{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmqedFJGF6cv",
        "outputId": "a7445d44-1a21-4599-c9d0-7024d3183e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 1.3422716387362659\n",
            "Epoch 2/3, Loss: 0.020268445155713435\n",
            "Epoch 3/3, Loss: 0.004971795022535005\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00        11\n",
            "           2       1.00      1.00      1.00         6\n",
            "           3       1.00      1.00      1.00        11\n",
            "           4       1.00      1.00      1.00         8\n",
            "           5       1.00      1.00      1.00         6\n",
            "           6       1.00      1.00      1.00         9\n",
            "           7       1.00      1.00      1.00         7\n",
            "           8       1.00      1.00      1.00        12\n",
            "           9       1.00      1.00      1.00        11\n",
            "          10       1.00      1.00      1.00        12\n",
            "          11       1.00      1.00      1.00         8\n",
            "          12       1.00      1.00      1.00        15\n",
            "          13       1.00      1.00      1.00         9\n",
            "          14       1.00      1.00      1.00        11\n",
            "          15       1.00      1.00      1.00         6\n",
            "          16       1.00      1.00      1.00         6\n",
            "          17       1.00      1.00      1.00         8\n",
            "          18       1.00      1.00      1.00         6\n",
            "          19       1.00      1.00      1.00         5\n",
            "          20       1.00      1.00      1.00        12\n",
            "          21       1.00      1.00      1.00         8\n",
            "          22       1.00      1.00      1.00        11\n",
            "          23       1.00      1.00      1.00        12\n",
            "          24       1.00      1.00      1.00         6\n",
            "          25       1.00      1.00      1.00        10\n",
            "          26       1.00      1.00      1.00         7\n",
            "          27       1.00      1.00      1.00         9\n",
            "          28       1.00      1.00      1.00         7\n",
            "          29       1.00      1.00      1.00        10\n",
            "          30       1.00      1.00      1.00        10\n",
            "          31       1.00      1.00      1.00         3\n",
            "          32       1.00      1.00      1.00         8\n",
            "          33       1.00      1.00      1.00        12\n",
            "          34       1.00      1.00      1.00         5\n",
            "          35       1.00      1.00      1.00        15\n",
            "          36       1.00      1.00      1.00        11\n",
            "          37       1.00      1.00      1.00         8\n",
            "          38       1.00      1.00      1.00        12\n",
            "          39       1.00      1.00      1.00         6\n",
            "          40       1.00      1.00      1.00        10\n",
            "          41       1.00      1.00      1.00         6\n",
            "          42       1.00      1.00      1.00        10\n",
            "          43       1.00      1.00      1.00         7\n",
            "          44       1.00      1.00      1.00         8\n",
            "          45       1.00      1.00      1.00        10\n",
            "          46       1.00      1.00      1.00         7\n",
            "          47       1.00      1.00      1.00        10\n",
            "          48       1.00      1.00      1.00         9\n",
            "          49       1.00      1.00      1.00         9\n",
            "          50       1.00      1.00      1.00         8\n",
            "          51       1.00      1.00      1.00         7\n",
            "          52       1.00      1.00      1.00        11\n",
            "          53       1.00      1.00      1.00        13\n",
            "          54       1.00      1.00      1.00         9\n",
            "          55       1.00      1.00      1.00        18\n",
            "          56       1.00      1.00      1.00        13\n",
            "          57       1.00      1.00      1.00        14\n",
            "          58       1.00      1.00      1.00         6\n",
            "          59       1.00      1.00      1.00        10\n",
            "          60       1.00      1.00      1.00        10\n",
            "          61       1.00      1.00      1.00         8\n",
            "          62       1.00      1.00      1.00         5\n",
            "          63       1.00      1.00      1.00         9\n",
            "          64       1.00      1.00      1.00         9\n",
            "          65       1.00      1.00      1.00         9\n",
            "          66       1.00      1.00      1.00        10\n",
            "          67       1.00      1.00      1.00         7\n",
            "          68       1.00      1.00      1.00         6\n",
            "          69       1.00      1.00      1.00         9\n",
            "          70       1.00      1.00      1.00         9\n",
            "          71       1.00      1.00      1.00        10\n",
            "          72       1.00      1.00      1.00         7\n",
            "          73       1.00      1.00      1.00         7\n",
            "          74       1.00      1.00      1.00        12\n",
            "          75       1.00      1.00      1.00        12\n",
            "          76       1.00      1.00      1.00        11\n",
            "          77       1.00      1.00      1.00        11\n",
            "          78       1.00      1.00      1.00         6\n",
            "          79       1.00      1.00      1.00        12\n",
            "          80       1.00      1.00      1.00         8\n",
            "          81       1.00      1.00      1.00         8\n",
            "          82       1.00      1.00      1.00         8\n",
            "          83       1.00      1.00      1.00        12\n",
            "          84       1.00      1.00      1.00        14\n",
            "          85       1.00      1.00      1.00        10\n",
            "          86       1.00      1.00      1.00         6\n",
            "          87       1.00      1.00      1.00         8\n",
            "          88       1.00      1.00      1.00         7\n",
            "          89       1.00      1.00      1.00         7\n",
            "          90       1.00      1.00      1.00        11\n",
            "          91       1.00      1.00      1.00         8\n",
            "          92       1.00      1.00      1.00         9\n",
            "          93       1.00      1.00      1.00        13\n",
            "          94       1.00      1.00      1.00        14\n",
            "          95       1.00      1.00      1.00         9\n",
            "          96       1.00      1.00      1.00        11\n",
            "          97       1.00      1.00      1.00        14\n",
            "          98       1.00      1.00      1.00        14\n",
            "          99       1.00      1.00      1.00         8\n",
            "         100       1.00      1.00      1.00         8\n",
            "         101       1.00      1.00      1.00         9\n",
            "         102       1.00      1.00      1.00        13\n",
            "         103       1.00      1.00      1.00        14\n",
            "         104       1.00      1.00      1.00         8\n",
            "         105       1.00      1.00      1.00        14\n",
            "         106       1.00      1.00      1.00         7\n",
            "         107       1.00      1.00      1.00         6\n",
            "         108       1.00      1.00      1.00         9\n",
            "         109       1.00      1.00      1.00         9\n",
            "         110       1.00      1.00      1.00         7\n",
            "         111       1.00      1.00      1.00         7\n",
            "         112       1.00      1.00      1.00         7\n",
            "         113       1.00      1.00      1.00         8\n",
            "         114       1.00      1.00      1.00         8\n",
            "         115       1.00      1.00      1.00         9\n",
            "         116       1.00      1.00      1.00         7\n",
            "         117       1.00      1.00      1.00        18\n",
            "         118       1.00      1.00      1.00         5\n",
            "         119       1.00      1.00      1.00        13\n",
            "         120       1.00      1.00      1.00         8\n",
            "         121       1.00      1.00      1.00        13\n",
            "         122       1.00      1.00      1.00         8\n",
            "         123       1.00      1.00      1.00        12\n",
            "         124       1.00      1.00      1.00         9\n",
            "         125       1.00      1.00      1.00        12\n",
            "         126       1.00      1.00      1.00         9\n",
            "         127       1.00      1.00      1.00        12\n",
            "         128       1.00      1.00      1.00         8\n",
            "         129       1.00      1.00      1.00        16\n",
            "         130       1.00      1.00      1.00         8\n",
            "         131       1.00      1.00      1.00         8\n",
            "         132       1.00      1.00      1.00        14\n",
            "         133       1.00      1.00      1.00         6\n",
            "         134       1.00      1.00      1.00        10\n",
            "         135       1.00      1.00      1.00         8\n",
            "         136       1.00      1.00      1.00         7\n",
            "         137       1.00      1.00      1.00         8\n",
            "         138       1.00      1.00      1.00        11\n",
            "         139       1.00      1.00      1.00        15\n",
            "         140       1.00      1.00      1.00         9\n",
            "         141       1.00      1.00      1.00        12\n",
            "         142       1.00      1.00      1.00         9\n",
            "         143       1.00      1.00      1.00         9\n",
            "         144       1.00      1.00      1.00        13\n",
            "         145       1.00      1.00      1.00         7\n",
            "         146       1.00      1.00      1.00         5\n",
            "         147       1.00      1.00      1.00         8\n",
            "         148       1.00      1.00      1.00         9\n",
            "         149       1.00      1.00      1.00        19\n",
            "         150       1.00      1.00      1.00        11\n",
            "         151       1.00      1.00      1.00        12\n",
            "         152       1.00      1.00      1.00        15\n",
            "         153       1.00      1.00      1.00        10\n",
            "         154       1.00      1.00      1.00        16\n",
            "         155       1.00      1.00      1.00         8\n",
            "         156       1.00      1.00      1.00        12\n",
            "         157       1.00      1.00      1.00        14\n",
            "         158       1.00      1.00      1.00         9\n",
            "         159       1.00      1.00      1.00         9\n",
            "         160       1.00      1.00      1.00         6\n",
            "         161       1.00      1.00      1.00         9\n",
            "         162       1.00      1.00      1.00         8\n",
            "         163       1.00      1.00      1.00        12\n",
            "         164       1.00      1.00      1.00         9\n",
            "         165       1.00      1.00      1.00         7\n",
            "         166       1.00      1.00      1.00        10\n",
            "         167       1.00      1.00      1.00        11\n",
            "         168       1.00      1.00      1.00        12\n",
            "         169       1.00      1.00      1.00        12\n",
            "\n",
            "    accuracy                           1.00      1632\n",
            "   macro avg       1.00      1.00      1.00      1632\n",
            "weighted avg       1.00      1.00      1.00      1632\n",
            "\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# import ssl\n",
        "# ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "# import torch\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# import os\n",
        "\n",
        "# # Load your CSV dataset\n",
        "# df = pd.read_csv('/content/benchmark.csv')\n",
        "\n",
        "# # Flatten the CSV into a list of data points\n",
        "# data_points = []\n",
        "# for col in df.columns:\n",
        "#     for value in df[col]:\n",
        "#         data_points.append(f\"{col}: {value}\")\n",
        "\n",
        "# # Create labels based on column names\n",
        "# labels = [col for col in df.columns for _ in range(len(df))]\n",
        "\n",
        "# # Encode labels using LabelEncoder\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data_points, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Tokenize the data with padding token\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "# X_train_tokens = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "# X_test_tokens = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "\n",
        "# # Create a custom dataset\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, tokens, labels):\n",
        "#         self.tokens = tokens\n",
        "#         self.labels = labels\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.labels)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return {\n",
        "#             'input_ids': self.tokens['input_ids'][idx],\n",
        "#             'attention_mask': self.tokens['attention_mask'][idx],\n",
        "#             'labels': torch.tensor(self.labels[idx])\n",
        "#         }\n",
        "\n",
        "# train_dataset = CustomDataset(X_train_tokens, y_train)\n",
        "# test_dataset = CustomDataset(X_test_tokens, y_test)\n",
        "\n",
        "# # Set up DataLoader\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# # Load pre-trained DistilBERT model for sequence classification\n",
        "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# # Set up optimizer and loss function using AdamW\n",
        "# optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# # Training loop\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "# num_epochs = 3\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "\n",
        "#     for batch in train_loader:\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     average_loss = total_loss / len(train_loader)\n",
        "#     print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}')\n",
        "\n",
        "# # Save the trained model\n",
        "# model_save_path = r'C:/Workspaces/CodeSpaces/Python_Work/Langchain_Model/saved_model'\n",
        "# os.makedirs(model_save_path, exist_ok=True)\n",
        "# model.save_pretrained(model_save_path)\n",
        "\n",
        "# # Evaluation\n",
        "# model.eval()\n",
        "# all_preds = []\n",
        "# all_labels = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for batch in test_loader:\n",
        "#         input_ids = batch['input_ids'].to(device)\n",
        "#         attention_mask = batch['attention_mask'].to(device)\n",
        "#         labels = batch['labels'].to(device)\n",
        "\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         logits = outputs.logits\n",
        "#         preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "#         all_preds.extend(preds.cpu().numpy())\n",
        "#         all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# # Decode predictions\n",
        "# decoded_preds = label_encoder.inverse_transform(all_preds)\n",
        "\n",
        "# # Evaluate the model\n",
        "# accuracy = accuracy_score(y_test, all_preds)\n",
        "# classification_rep = classification_report(y_test, all_preds)\n",
        "\n",
        "# # Display the results\n",
        "# print(classification_rep)\n",
        "# print(accuracy)\n",
        "\n",
        "\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Load your CSV dataset\n",
        "df = pd.read_csv('/content/benchmark.csv')\n",
        "\n",
        "# Flatten the CSV into a list of data points\n",
        "data_points = []\n",
        "for col in df.columns:\n",
        "    for value in df[col]:\n",
        "        data_points.append(f\"{col}: {value}\")\n",
        "\n",
        "# Create labels based on column names\n",
        "labels = [col for col in df.columns for _ in range(len(df))]\n",
        "\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "label_encoder_file = '/content/label_encoder.npy'\n",
        "np.save(label_encoder_file, label_encoder.classes_)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_points, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the data with padding token\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "X_train_tokens = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "X_test_tokens = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "\n",
        "# Create a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.tokens = tokens\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokens['input_ids'][idx],\n",
        "            'attention_mask': self.tokens['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(X_train_tokens, y_train)\n",
        "test_dataset = CustomDataset(X_test_tokens, y_test)\n",
        "\n",
        "# Set up DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Load pre-trained DistilBERT model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Set up optimizer and loss function using AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}')\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "model_save_path = '/content/saved_model'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Decode predictions\n",
        "decoded_preds = label_encoder.inverse_transform(all_preds)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, all_preds)\n",
        "classification_rep = classification_report(y_test, all_preds)\n",
        "\n",
        "# Display the results\n",
        "print(classification_rep)\n",
        "print(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFvmTMB1lGX",
        "outputId": "09f11e3b-10fa-4a6a-de75-fc9c62d7cef9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Load your CSV dataset\n",
        "df = pd.read_csv('/content/benchmark.csv')\n",
        "\n",
        "# Flatten the CSV into a list of data points\n",
        "data_points = []\n",
        "for index, row in df.iterrows():\n",
        "    for col, value in zip(df.columns, row):\n",
        "        data_points.append(f\"{col}: {value}\")\n",
        "\n",
        "# Create labels based on column names\n",
        "labels = [col for col in df.columns for _ in range(len(df))]\n",
        "\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "label_encoder_file = '/content/label_encoder.npy'\n",
        "np.save(label_encoder_file, label_encoder.classes_)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_points, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize the data with padding token\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "X_train_tokens = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "X_test_tokens = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt', max_length=50)\n",
        "\n",
        "# Create a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, tokens, labels):\n",
        "        self.tokens = tokens\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.tokens['input_ids'][idx],\n",
        "            'attention_mask': self.tokens['attention_mask'][idx],\n",
        "            'labels': torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "train_dataset = CustomDataset(X_train_tokens, y_train)\n",
        "test_dataset = CustomDataset(X_test_tokens, y_test)\n",
        "\n",
        "# Set up DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Load pre-trained DistilBERT model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "# Set up optimizer and loss function using AdamW\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "num_epochs = 50\n",
        "\n",
        "# Before the training loop\n",
        "accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "      loss = outputs.loss\n",
        "\n",
        "      # Perform gradient accumulation\n",
        "      loss.backward()\n",
        "      if (i + 1) % accumulation_steps == 0 or i == len(train_loader) - 1:\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Adjust max_norm as needed\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}')\n",
        "\n",
        "# After the training loop\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "model_save_path = '/content/Model/saved_model'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Decode predictions\n",
        "decoded_preds = label_encoder.inverse_transform(all_preds)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, all_preds)\n",
        "classification_rep = classification_report(y_test, all_preds)\n",
        "\n",
        "# Display the results\n",
        "print(classification_rep)\n",
        "print(accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "QK69V5d_wlW1",
        "outputId": "d3e0866c-b704-46ab-d35e-f5cc0c446cdf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-06344b3941b8>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2269\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 )\n\u001b[0;32m-> 2271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 30.81 MiB is free. Process 2050 has 14.71 GiB memory in use. Of the allocated memory 13.77 GiB is allocated by PyTorch, and 138.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# import numpy as np  # Import NumPy for loading the label encoder\n",
        "\n",
        "# # Load the trained model and tokenizer\n",
        "# model_path = '/content/saved_model'\n",
        "# model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model.to(device)\n",
        "\n",
        "# # Load your CSV dataset for inference\n",
        "# df_inference = pd.read_csv('/content/AscotBDX08092Main.csv')  # Replace with the path to your inference dataset\n",
        "\n",
        "# # Load the label encoder from the saved file\n",
        "# label_encoder_file = '/content/label_encoder.npy'\n",
        "# label_encoder = LabelEncoder()\n",
        "# label_encoder.classes_ = np.load(label_encoder_file)\n",
        "\n",
        "# # Tokenize the new data\n",
        "# max_batch_size = 2  # Adjust this based on your available GPU memory\n",
        "# start_idx = 0\n",
        "\n",
        "# # Store the predictions in a list\n",
        "# predictions = []\n",
        "\n",
        "# while start_idx < len(df_inference.columns):\n",
        "#     end_idx = start_idx + max_batch_size\n",
        "#     batch_data = list(df_inference.columns[start_idx:end_idx])\n",
        "\n",
        "#     new_data_tokens = tokenizer(batch_data, padding=True, truncation=True, return_tensors='pt', max_length=50).to(device)\n",
        "\n",
        "#     # Make predictions\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         # Move data to GPU if available\n",
        "#         new_data_tokens = {key: value.to(device) for key, value in new_data_tokens.items()}\n",
        "\n",
        "#         outputs = model(**new_data_tokens)\n",
        "#         logits = outputs.logits\n",
        "#         preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "#     # Decode predictions\n",
        "#     decoded_preds = label_encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "#     # Store the predictions in the list\n",
        "#     predictions.extend(zip(batch_data, decoded_preds))\n",
        "\n",
        "#     # Clear GPU cache\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "#     start_idx = end_idx\n",
        "\n",
        "# # Display predictions\n",
        "# print(\"Predicted Mapping:\", predictions)\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np  # Import NumPy for loading the label encoder\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = '/content/saved_model'\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load your CSV dataset for inference\n",
        "df_inference = pd.read_csv('/content/AscotBDX08092Main.csv')  # Replace with the path to your inference dataset\n",
        "\n",
        "# Assuming your data is in the same format as during training\n",
        "data_points_inference = []\n",
        "column_predictions = []\n",
        "\n",
        "for col in df_inference.columns:\n",
        "    col_predictions = {\"inference_column\": col, \"predicted_trained_column\": []}\n",
        "    for value in df_inference[col]:\n",
        "        data_point = f\"{col}: {value}\"\n",
        "        data_points_inference.append(data_point)\n",
        "\n",
        "        # Tokenize the new data\n",
        "        new_data_tokens = tokenizer(data_point, padding=True, truncation=True, return_tensors='pt', max_length=50).to(device)\n",
        "\n",
        "        # Make predictions\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Move data to GPU if available\n",
        "            new_data_tokens = {key: value.to(device) for key, value in new_data_tokens.items()}\n",
        "            outputs = model(**new_data_tokens)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Decode predictions\n",
        "        decoded_preds = label_encoder.inverse_transform(preds.cpu().numpy())\n",
        "        col_predictions[\"predicted_trained_column\"].append(decoded_preds[0])\n",
        "\n",
        "        # Clear GPU cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    column_predictions.append(col_predictions)\n",
        "\n",
        "# Display predictions\n",
        "for prediction in column_predictions:\n",
        "    print(prediction)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "bsNegqDObTCe",
        "outputId": "fc3edff3-1597-4882-c730-0df97bda5a77"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-794d0c10a695>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Move data to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mnew_data_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_data_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnew_data_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         return self.transformer(\n\u001b[0m\u001b[1;32m    600\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 )\n\u001b[1;32m    368\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \"\"\"\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Self-Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         sa_output = self.attention(\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, q_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_lin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, n_heads, k_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;34m\"\"\"separate heads\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0munshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np  # Import NumPy for loading the label encoder\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = '/content/Model/saved_model'\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Load your CSV dataset for inference\n",
        "df_inference = pd.read_csv('/content/AscotBDX08092Main.csv')  # Replace with the path to your inference dataset\n",
        "\n",
        "# Load the label encoder from the saved file\n",
        "label_encoder_file = '/content/label_encoder.npy'\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.classes_ = np.load(label_encoder_file)\n",
        "\n",
        "# Tokenize the new data\n",
        "max_batch_size = 2  # Adjust this based on your available GPU memory\n",
        "start_idx = 0\n",
        "\n",
        "# Initialize a set to keep track of processed columns\n",
        "processed_columns = set()\n",
        "\n",
        "# Initialize a list to capture the mapping information\n",
        "mapping_info = []\n",
        "\n",
        "while start_idx < len(df_inference.columns):\n",
        "    end_idx = start_idx + max_batch_size\n",
        "    batch_data = [\": \".join([df_inference.columns[i], str(df_inference.iloc[0, i])]) for i in range(start_idx, end_idx)]\n",
        "\n",
        "    new_data_tokens = tokenizer(batch_data, padding=True, truncation=True, return_tensors='pt', max_length=50).to(device)\n",
        "\n",
        "    # Make predictions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Move data to GPU if available\n",
        "        new_data_tokens = {key: value.to(device) for key, value in new_data_tokens.items()}\n",
        "\n",
        "        outputs = model(**new_data_tokens)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "    # Decode predictions\n",
        "    decoded_preds = label_encoder.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "    # Capture the mapping information for each unique column\n",
        "    for i, column_name in enumerate(batch_data):\n",
        "        if column_name not in processed_columns:\n",
        "            mapping_info.append({f\"Column from Inference CSV\": column_name, \"Predicted Class Name\": decoded_preds[i]})\n",
        "            processed_columns.add(column_name)\n",
        "\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    start_idx = end_idx\n",
        "\n",
        "# Display the final mapping information\n",
        "print(mapping_info)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6DoIVP6tjsl",
        "outputId": "30386e58-59d1-46fb-9e20-3caf606463d5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'Column from Inference CSV': 'uploadid: 497', 'Predicted Class Name': 'Contents Value'}, {'Column from Inference CSV': 'reporting_period: 92023', 'Predicted Class Name': 'Square Footage'}, {'Column from Inference CSV': 'policy_number: ASC-2977825-00', 'Predicted Class Name': 'Full policy number'}, {'Column from Inference CSV': 'expiring_policy_number: nan', 'Predicted Class Name': 'Expiring Policy Number'}, {'Column from Inference CSV': 'policy_effective_date: 05-09-2023', 'Predicted Class Name': 'Policy Effective Date'}, {'Column from Inference CSV': 'policy_expiration_date: 05-09-2024', 'Predicted Class Name': 'Policy Expiration Date'}, {'Column from Inference CSV': 'transaction_effective_date: 01-09-2023', 'Predicted Class Name': 'Transaction Effective Date'}, {'Column from Inference CSV': 'transaction_expiration_date: 05-09-2024', 'Predicted Class Name': 'Transaction Expiration Date'}, {'Column from Inference CSV': 'transaction_type: NB', 'Predicted Class Name': 'Transaction Type'}, {'Column from Inference CSV': 'transaction_number: 0', 'Predicted Class Name': 'Transaction Type'}, {'Column from Inference CSV': 'transaction_description: New Business', 'Predicted Class Name': 'Class Code Description'}, {'Column from Inference CSV': 'coverage: AL', 'Predicted Class Name': 'Coverage Symbol'}, {'Column from Inference CSV': 'written_premium_ex_tria:  $14,268.25 ', 'Predicted Class Name': 'Premium'}, {'Column from Inference CSV': 'mga_participation: 0%', 'Predicted Class Name': 'MGA'}, {'Column from Inference CSV': 'ascot_share: 100', 'Predicted Class Name': 'CM/Occ Indicator'}, {'Column from Inference CSV': 'exposure: 1.0', 'Predicted Class Name': 'Exposure Amount'}, {'Column from Inference CSV': 'exposure_basis: Vehicles', 'Predicted Class Name': 'Exposure Type'}, {'Column from Inference CSV': 'line_of_business: Auto Liability', 'Predicted Class Name': 'Class Code Description'}, {'Column from Inference CSV': 'limit_occurrence: 1000000', 'Predicted Class Name': 'Policy Occurrence Limit'}, {'Column from Inference CSV': 'limit_aggregate: $2,000,000', 'Predicted Class Name': 'Policy Aggregate Limit'}, {'Column from Inference CSV': 'attachment_point: 0', 'Predicted Class Name': 'Attachment Point'}, {'Column from Inference CSV': 'deductible: nan', 'Predicted Class Name': 'AOP Deductible'}, {'Column from Inference CSV': 'commission:  $2,496.94 ', 'Predicted Class Name': 'Commission amount'}, {'Column from Inference CSV': 'schedule_modifier: nan', 'Predicted Class Name': 'Class Code Description'}, {'Column from Inference CSV': 'risk_location_address_1: 21400 DAWES ST', 'Predicted Class Name': 'Risk/Location Address'}, {'Column from Inference CSV': 'risk_location_address_2: nan', 'Predicted Class Name': 'Risk/Location Address'}, {'Column from Inference CSV': 'risk_state_code: CA', 'Predicted Class Name': 'Risk/Location State'}, {'Column from Inference CSV': 'risk_location_zip: 92570', 'Predicted Class Name': 'Risk/Location Zip'}, {'Column from Inference CSV': 'insured_name: ELIAZAR MORALES', 'Predicted Class Name': 'Insured Name'}, {'Column from Inference CSV': 'insured_address_1: 21400 DAWES ST', 'Predicted Class Name': 'Insured Address'}, {'Column from Inference CSV': 'insured_address_2: nan', 'Predicted Class Name': 'Insured Address'}, {'Column from Inference CSV': 'insured_city: PERRIS', 'Predicted Class Name': 'Insured City'}, {'Column from Inference CSV': 'insured_state: CA', 'Predicted Class Name': 'Insured State'}, {'Column from Inference CSV': 'insured_zip: 92570-9214', 'Predicted Class Name': 'Insured Zip'}, {'Column from Inference CSV': 'admitted_nonadmitted: Non Admitted', 'Predicted Class Name': 'Direct/Assumed'}, {'Column from Inference CSV': 'producer_individual_name: Cover Whale Insurance Solutions Inc.', 'Predicted Class Name': 'Insured Name'}, {'Column from Inference CSV': 'producer_individual_license_number: 0M87896', 'Predicted Class Name': 'SL License Number'}, {'Column from Inference CSV': 'producer_firm_license_number: 0M87896', 'Predicted Class Name': 'Broker/Agent Address'}, {'Column from Inference CSV': 'producer_firm_name: Daniel Abrahamsen', 'Predicted Class Name': 'Insured Name'}, {'Column from Inference CSV': 'producer_address_1: 9584 Greensburg Ave', 'Predicted Class Name': 'Risk/Location Address'}, {'Column from Inference CSV': 'producer_address_2: nan', 'Predicted Class Name': 'Risk/Location Address'}, {'Column from Inference CSV': 'producer_city: Las Vegas', 'Predicted Class Name': 'Broker/Agent City'}, {'Column from Inference CSV': 'producer_state: Nevada', 'Predicted Class Name': 'SL Broker State'}, {'Column from Inference CSV': 'producer_zip: 89178', 'Predicted Class Name': 'Insured Zip'}, {'Column from Inference CSV': 'nj_sla_number: nan', 'Predicted Class Name': 'NJ Transaction Number'}, {'Column from Inference CSV': 'primary_excess: Primary', 'Predicted Class Name': 'Primary/Excess Indicator'}, {'Column from Inference CSV': 'claims_basis: Occurrence', 'Predicted Class Name': 'SIR/Deductible type'}, {'Column from Inference CSV': 'rating_factor_base_rate: 4250.0', 'Predicted Class Name': 'Rating Class Code'}, {'Column from Inference CSV': 'rating_factor_experience: 1.3', 'Predicted Class Name': 'Rating Class Code'}, {'Column from Inference CSV': 'rating_driving_radius: 1.4565', 'Predicted Class Name': 'Rating Class Code'}, {'Column from Inference CSV': 'rating_fleet_nonfleet: 0.94', 'Predicted Class Name': 'Rating Class Code'}, {'Column from Inference CSV': 'rating_seating_capacity: 01-Aug', 'Predicted Class Name': 'Vehicle Number of Seats, Seating Capacity'}, {'Column from Inference CSV': 'rating_gross_vehicle_weight: Extra Heavy Trucks (45,001+)', 'Predicted Class Name': 'Gross Vehicle Weight'}, {'Column from Inference CSV': 'rate: 14268.25', 'Predicted Class Name': 'Commission amount'}, {'Column from Inference CSV': 'vin: 3AKJGLDV6JSJX7317', 'Predicted Class Name': 'MTC Deductible Amount'}, {'Column from Inference CSV': 'vehicle_make: FREIGHTLINER', 'Predicted Class Name': 'Vehicle Make'}, {'Column from Inference CSV': 'vehicle_model: CASCADIA', 'Predicted Class Name': 'Vehicle Model'}, {'Column from Inference CSV': 'vehicle_year: 2018', 'Predicted Class Name': 'Year Built'}, {'Column from Inference CSV': 'vehicle_type: Truck', 'Predicted Class Name': 'Vehicle Type'}, {'Column from Inference CSV': 'limit_medpay: 5000.0', 'Predicted Class Name': '100% Limit'}, {'Column from Inference CSV': 'limit_pip: nan', 'Predicted Class Name': '100% Limit'}, {'Column from Inference CSV': 'historical_loss_years: 2.0', 'Predicted Class Name': 'Data and software loss'}, {'Column from Inference CSV': 'historical_loss_count: nan', 'Predicted Class Name': 'Risk/Location City'}, {'Column from Inference CSV': 'historical_loss_amount: nan', 'Predicted Class Name': 'Risk/Location City'}]\n"
          ]
        }
      ]
    }
  ]
}