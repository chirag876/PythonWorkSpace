{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = r'C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\Mapping.json'\n",
    "# Read raw JSON data\n",
    "with open(json_file_path, 'r') as json_file_path:\n",
    "    json_data = json.load(json_file_path)\n",
    "# Print the raw JSON data\n",
    "print(json.dumps(json_data, indent=2)) # Use indent for pretty printing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MGA', 'Issuing Company', 'Full policy number', 'Insured Name']\n",
      "['Arrowhead General Insurance Agency Inc', 'TSIC', 'TSAHGL0000025-00', 'Rego Realty Corporation']\n",
      "['Arrowhead General Insurance Agency Inc', 'TSIC', 'TSAHGL0000050-00', 'J Sterling Quality Roofing, Inc.']\n",
      "['Arrowhead General Insurance Agency Inc', 'TSIC', 'TSAHGL0000050-00', 'J Sterling Quality Roofing, Inc.']\n",
      "['Plimsoll Specialty Markets LLC', 'Transverse Specialty Insurance Company', 'TSBAAC 0000002-00', '3D Aviation Inc']\n",
      "['Marsh USA Chicago, Inc., ', 'Transverse Specialty Insurance Company', 'TSBAAC 0000091-00', 'Venture Travel, LLC']\n",
      "['Marsh USA Chicago, Inc., ', 'Transverse Specialty Insurance Company', 'TSBAAG 0000092-00', 'Venture Travel, LLC']\n",
      "['Marsh & McLennan Agency', 'Transverse Specialty Insurance Company', 'TSBAAC 0000086-00', 'Gateway Canyons Air Tours, LLC']\n",
      "['Marsh & McLennan Agency', 'Transverse Specialty Insurance Company', 'TSBAAC 0000086-00', 'Gateway Canyons Air Tours, LLC']\n",
      "['Travers & Associates', 'Transverse Insurance Company', 'TIBAAN 0000037-00', 'Shoreline Aviation LLC']\n",
      "['Travers & Associates', 'Transverse Insurance Company', 'TIBAAN 0000037-00', 'Shoreline Aviation LLC']\n",
      "['Travers & Associates', 'Transverse Insurance Company', 'TIBAAN 0000037-00', 'Shoreline Aviation LLC']\n",
      "['Wings Insurance ', 'Transverse Insurance Company', 'TIBAAN 0000067-00', 'ZMN Enterprises, LLC and Centurion Holdings, LLC']\n",
      "['Halton Hall & Associates, Inc.', 'Transverse Insurance Company', 'TIBAAN 0000080-00', 'Pollard Aircraft Sales Inc; Pollard Spares LLC; et al']\n",
      "['Halton Hall & Associates, Inc.', 'Transverse Insurance Company', 'TIBAAN 0000080-00', 'Pollard Aircraft Sales Inc; Pollard Spares LLC; et al']\n",
      "['Wings Insurance ', 'Transverse Insurance Company', 'TIBAAN 000093-00', 'Rafter K LLC']\n",
      "['Wings Insurance ', 'Transverse Insurance Company', 'TIBAAN 0009002-00', 'Kupau Inc']\n",
      "['Travers & Associates', 'Transverse Insurance Company', 'TIBAAN 0009003-00', 'DILO Equipment Leasing, LLC']\n",
      "['Wings Insurance ', 'Transverse Insurance Company', 'TIBAAN 0009000-00', 'N37GD, LLC']\n",
      "['Avion Insurance Agency, Inc', 'Transverse Insurance Company', 'TIBAAN 0009005-00', 'Angely Air, LLC']\n"
     ]
    }
   ],
   "source": [
    "# input_data_df = pd.read_csv('input_data.csv')\n",
    "input_csv = r'C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\test.csv'\n",
    "with open(input_csv, 'r') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    # Iterate through the rows and print each row\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# file_path = r'C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\Mapping.json'\n",
    "\n",
    "# # Read JSON data into a DataFrame\n",
    "# df = pd.read_json(file_path, orient='records')  # Set 'orient' based on your JSON structure\n",
    "\n",
    "# # Print the DataFrame\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r'C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\Mapping.json'\n",
    "\n",
    "# def read_csv(file_path):\n",
    "#     return pd.read_json(file_path, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(df, alt_names):\n",
    "    pairs = []\n",
    "    confidence_scores = [100, 0]\n",
    "\n",
    "    for ind, column1 in enumerate(df.columns):\n",
    "        i = 0\n",
    "        for index, column in enumerate(df.columns):\n",
    "            if column1 == column:\n",
    "                if column1 == column:\n",
    "                    prompt = f\"\"\"Your task is to classify and return a confidence score for the {column} column with the {column1} column. This is your target column: {column1}. If the {column} column name is similar to {alt_names[ind]}, then the confidence score is {confidence_scores[i]}. Also, if the data patterns, data types, common values, or trends in the {column} column ({list(df.iloc[:, index].values)}) are similar to {list(df.iloc[:, ind].values)} data, then the confidence score is {confidence_scores[i]}. Now, based on the analysis, generate the output\\n\\n##\\n\\n\"\"\"\n",
    "                    completion = f\"\"\"{confidence_scores[i]},{column}, {column1} \\n\\n##\\n\\n\"\"\"\n",
    "                    pairs.append({\"prompt\": prompt, \"completion\": completion})\n",
    "                else:\n",
    "                    prompt = f\"\"\"Your task is to classify and return a confidence score for the {column} column with the {column1} column. This is your target column: {column1}. If the {column} column name is not similar to {alt_names[index]}, then the confidence score is {confidence_scores[i+1]}. Also, if the data patterns, data types, common values, or trends in the {column} column ({list(df.iloc[:, index].values)}) are not similar to {list(df.iloc[:, ind].values)} data, then the confidence score is {confidence_scores[i+1]}. Now, based on the analysis, generate the output\\n\\n##\\n\\n\"\"\"\n",
    "                    completion = f\"\"\"{confidence_scores[i+1]},{column} , {column1}\\n\\n##\\n\\n\"\"\"\n",
    "                    pairs.append({\"prompt\": prompt, \"completion\": completion})\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pairs_to_json(pairs, file_path):\n",
    "    random.shuffle(pairs)\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df = pd.read_csv('input_data.csv')\n",
    "alt_names_df = pd.read_csv('alternative.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_names = alt_names_df.iloc[:, 1].tolist()\n",
    "\n",
    "# Create pairs\n",
    "pairs = create_pairs(input_data_df, alt_names)\n",
    "\n",
    "# Save pairs to JSON\n",
    "save_pairs_to_json(pairs, \"new_prompt_64.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'your_api_key'  # Replace with your actual API key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "try:\n",
    "    response = client.files.create(\n",
    "        file=open(\"new_prompt_64.json\", \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "    file_id = response.id\n",
    "    print(\"File ID:\", file_id)\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error during file upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = response.model\n",
    "\n",
    "# Create a fine-tuning job\n",
    "url = \"https://api.openai.com/v1/fine_tuning/jobs\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "payload = {\"training_file\": file_id, \"model\": model_id}\n",
    "\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    job_id = response.json()[\"id\"]\n",
    "    print(\"Fine-tuning job created. Job ID:\", job_id)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating fine-tuning job: {e}\")\n",
    "\n",
    "# Wait for the fine-tuning job to complete\n",
    "while True:\n",
    "    url = f\"https://api.openai.com/v1/fine_tuning/jobs/{job_id}\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        status = response.json()[\"status\"]\n",
    "        print(\"Fine-tuning status:\", status)\n",
    "        if status == \"succeeded\":\n",
    "            break  # Exit the loop if fine-tuning is complete\n",
    "        elif status == \"failed\":\n",
    "            print(\"Fine-tuning job failed.\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking fine-tuning status: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = f\"ft:{model_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = r\"C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\test.csv\"\n",
    "benchmark = r\"C:\\Workspaces\\CodeSpaces\\Python_Work\\ChatGpt_AzureAI\\21_12_23\\21_12_8.csv\"\n",
    "test_df = pd.read_csv(test_data)\n",
    "bench = pd.read_csv(benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in test_df.columns:\n",
    "    for col in bench.columns:\n",
    "        payload = {\n",
    "            \"prompt\": f\"\"\"Your task is to classify and return a confidence_score for the given {column} column with the {col} column. This is your target column: {col}. {column} data is {list(test_df[column].values)} and {col} data is {list(bench[col].values)}.Now, based on the analysis, generate the output\\n\\n##\\n\\n\"\"\",\n",
    "            \"temperature\": 0.02,\n",
    "            \"max_tokens\": 15,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(f\"https://api.openai.com/v1/engines/{model}/completions\", headers=headers, json=payload)\n",
    "            result = response.json()\n",
    "            confidence_score = result[\"choices\"][0][\"text\"].split(',')[0].strip()\n",
    "            print(f\"{confidence_score}, {column}, {col}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating confidence score: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
